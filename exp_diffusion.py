"""Experimental diffusion based text to image generation.

References:
    - https://github.com/quickgrid/paper-implementations/blob/main/pytorch/vision_transformer/vit.py
    - https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py
    - https://github.com/lucidrains/memory-efficient-attention-pytorch
    - https://github.com/quickgrid/pytorch-diffusion
    - Self Attention Paper, https://arxiv.org/pdf/1706.03762.pdf
    - ViT Paper, https://arxiv.org/pdf/2010.11929.pdf
"""
import logging
import math
import os
from typing import Tuple, Union, List

import numpy as np
import torch
import torchvision
from PIL import Image
from torch.cuda.amp import GradScaler
from torch.functional import F
from torch import nn, optim
from einops.layers.torch import Rearrange
from memory_efficient_attention_pytorch.flash_attention import FlashAttention
from torch.utils.data import Dataset
from torchvision.transforms import transforms
from tqdm import tqdm


class DiffusionDDPM:
    def __init__(
            self,
            device: str,
            img_size: int,
            noise_steps: int = 1000,
            beta_start: float = 1e-4,
            beta_end: float = 0.02,
    ):
        self.device = device
        self.noise_steps = noise_steps
        self.beta_start = beta_start
        self.beta_end = beta_end
        self.img_size = img_size

        # Section 2, equation 4 and near explation for alpha, alpha hat, beta.
        self.beta = self.linear_noise_schedule()
        self.alpha = 1 - self.beta
        self.alpha_hat = torch.cumprod(self.alpha, dim=0)

        # Section 3.2, algorithm 1 formula implementation. Generate values early reuse later.
        self.sqrt_alpha_hat = torch.sqrt(self.alpha_hat)
        self.sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat)

        # Section 3.2, equation 2 precalculation values.
        self.sqrt_alpha = torch.sqrt(self.alpha)
        self.std_beta = torch.sqrt(self.beta)

        # Clean up unnecessary values.
        del self.alpha
        del self.alpha_hat

    def linear_noise_schedule(self) -> torch.Tensor:
        """Same amount of noise is applied each step. Weakness is near end steps image is so noisy it is hard make
        out information. So noise removal is also very small amount, so it takes more steps to generate clear image.
        """
        return torch.linspace(start=self.beta_start, end=self.beta_end, steps=self.noise_steps, device=self.device)

    def q_sample(self, x: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Section 3.2, algorithm 1 formula implementation. Forward process, defined by `q`.

        Found in section 2. `q` gradually adds gaussian noise according to variance schedule. Also,
        can be seen on figure 2.
        """
        sqrt_alpha_hat = self.sqrt_alpha_hat[t].view(-1, 1, 1, 1)
        sqrt_one_minus_alpha_hat = self.sqrt_one_minus_alpha_hat[t].view(-1, 1, 1, 1)
        epsilon = torch.randn_like(x, device=self.device)
        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * epsilon, epsilon

    def sample_timesteps(self, batch_size: int) -> torch.Tensor:
        """Random timestep for each sample in a batch. Timesteps selected from [1, noise_steps].
        """
        return torch.randint(low=1, high=self.noise_steps, size=(batch_size,), device=self.device)

    def p_sample(
            self,
            eps_model: nn.Module,
            n: int,
            scale_factor: int = 2,
            conditional_embedding: torch.Tensor = None,
            contextual_text_embedding: torch.Tensor = None,
            contextual_text_mask: torch.Tensor = None,
    ) -> torch.Tensor:
        """Implementation of algorithm 2 sampling. Reverse process, defined by `p` in section 2. Short
         formula is defined in equation 11 of section 3.2.

        From noise generates image step by step. From noise_steps, (noise_steps - 1), ...., 2, 1.
        Here, alpha = 1 - beta. So, beta = 1 - alpha.

        Sample noise from normal distribution of timestep t > 1, else noise is 0. Before returning values
        are clamped to [-1, 1] and converted to pixel values [0, 255].

        Args:
            contextual_text_mask:
            contextual_text_embedding:
            conditional_embedding: Pooled embedding (same shape per string) generated by language model.
            scale_factor: Scales the output image by the factor.
            eps_model: Noise prediction model. `eps_theta(x_t, t)` in paper. Theta is the model parameters.
            n: Number of samples to process.

        Returns:
            Generated denoised image.
        """
        logging.info(f'Sampling {n} images....')

        eps_model.eval()
        with torch.no_grad():
            x = torch.randn((n, 3, self.img_size, self.img_size), device=self.device)

            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):
                t = torch.ones(n, dtype=torch.long, device=self.device) * i

                sqrt_alpha_t = self.sqrt_alpha[t].view(-1, 1, 1, 1)
                beta_t = self.beta[t].view(-1, 1, 1, 1)
                sqrt_one_minus_alpha_hat_t = self.sqrt_one_minus_alpha_hat[t].view(-1, 1, 1, 1)
                epsilon_t = self.std_beta[t].view(-1, 1, 1, 1)

                random_noise = torch.randn_like(x) if i > 1 else torch.zeros_like(x)

                x = ((1 / sqrt_alpha_t) *
                     (x - ((beta_t / sqrt_one_minus_alpha_hat_t) *
                           eps_model(
                               x=x,
                               t=t,
                               conditional_embedding=conditional_embedding,
                               contextual_text_embedding=contextual_text_embedding,
                               contextual_text_mask=contextual_text_mask,

                           )))
                     ) + (epsilon_t * random_noise)

        eps_model.train()

        x = ((x.clamp(-1, 1) + 1) * 127.5).type(torch.uint8)
        x = F.interpolate(input=x, scale_factor=scale_factor, mode='nearest-exact')
        return x


class DiffusionDDIM:
    def __init__(
            self,
            device: str,
            img_size: int,
            noise_steps: int,
            min_signal_rate: int = 0.02,
            max_signal_rate: int = 0.95,
    ):
        self.max_signal_rate = max_signal_rate
        self.min_signal_rate = min_signal_rate
        self.device = device
        self.noise_steps = noise_steps
        self.img_size = img_size

    def diffusion_schedule(
            self,
            diffusion_times,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        max_signal_rate = torch.tensor(self.max_signal_rate, dtype=torch.float, device=self.device)
        min_signal_rate = torch.tensor(self.min_signal_rate, dtype=torch.float, device=self.device)

        start_angle = torch.acos(max_signal_rate)
        end_angle = torch.acos(min_signal_rate)

        diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)

        signal_rates = torch.cos(diffusion_angles)
        noise_rates = torch.sin(diffusion_angles)

        return noise_rates, signal_rates

    @staticmethod
    def denoise(
            eps_model: nn.Module,
            noisy_images: torch.Tensor,
            noise_rates: torch.Tensor,
            signal_rates: torch.Tensor,
            training: bool = True
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Predict noise component and calculate the image component using it.
        """
        if training:
            pred_noises = eps_model(noisy_images, noise_rates.to(dtype=torch.long) ** 2)
            pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates
            return pred_noises, pred_images

        with torch.no_grad():
            pred_noises = eps_model(noisy_images, noise_rates.to(dtype=torch.long) ** 2)
            pred_images = (noisy_images - noise_rates * pred_noises) / signal_rates

        return pred_noises, pred_images

    def reverse_diffusion(
            self,
            num_images: int,
            diffusion_steps: int,
            eps_model: nn.Module,
            scale_factor: int = 2,
    ) -> Union[torch.Tensor, List[Image.Image]]:
        eps_model.eval()

        pred_images = None
        initial_noise = torch.randn((num_images, 3, self.img_size, self.img_size), device=self.device)

        step_size = 1.0 / diffusion_steps

        next_noisy_images = initial_noise
        for step in range(diffusion_steps):
            noisy_images = next_noisy_images

            diffusion_times = torch.ones((num_images, 1, 1, 1), device=self.device) - step * step_size
            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)
            pred_noises, pred_images = self.denoise(
                eps_model, noisy_images, noise_rates, signal_rates, training=False
            )

            next_diffusion_times = diffusion_times - step_size
            next_noise_rates, next_signal_rates = self.diffusion_schedule(next_diffusion_times)
            next_noisy_images = (next_signal_rates * pred_images + next_noise_rates * pred_noises)

        eps_model.train()

        pred_images = ((pred_images.clamp(-1, 1) + 1) * 127.5).type(torch.uint8)
        pred_images = F.interpolate(input=pred_images, scale_factor=scale_factor, mode='nearest-exact')
        return pred_images


class EMA:
    def __init__(self, beta):
        """Modifies exponential moving average model.
        """
        self.beta = beta
        self.step = 0

    def update_model_average(self, ema_model: nn.Module, current_model: nn.Module) -> None:
        for current_params, ema_params in zip(current_model.parameters(), ema_model.parameters()):
            old_weights, new_weights = ema_params.data, current_params.data
            ema_params.data = self.update_average(old_weights=old_weights, new_weights=new_weights)

    def update_average(self, old_weights: torch.Tensor, new_weights: torch.Tensor) -> torch.Tensor:
        if old_weights is None:
            return new_weights
        return old_weights * self.beta + (1 - self.beta) * new_weights

    def ema_step(self, ema_model: nn.Module, model: nn.Module, step_start_ema: int = 2000) -> None:
        if self.step < step_start_ema:
            self.reset_parameters(ema_model=ema_model, model=model)
            self.step += 1
            return
        self.update_model_average(ema_model=ema_model, current_model=model)
        self.step += 1

    @staticmethod
    def reset_parameters(ema_model: nn.Module, model: nn.Module) -> None:
        ema_model.load_state_dict(model.state_dict())


class Utils:
    def __init__(self):
        super(Utils, self).__init__()

    @staticmethod
    def collate_fn(batch):
        """Discard none samples.
        """
        batch = list(filter(lambda x: x is not None, batch))
        return torch.utils.data.dataloader.default_collate(batch)

    @staticmethod
    def save_images(images: torch.Tensor, save_path: str) -> None:
        grid = torchvision.utils.make_grid(images)
        img_arr = grid.permute(1, 2, 0).cpu().numpy()
        img = Image.fromarray(img_arr)
        img.save(save_path)

    @staticmethod
    def save_checkpoint(
            epoch: int,
            model: nn.Module,
            filename: str,
            optimizer: optim.Optimizer = None,
            scheduler: optim.lr_scheduler = None,
            grad_scaler: GradScaler = None,
    ) -> None:
        checkpoint = {
            'epoch': epoch,
            'state_dict': model.state_dict(),
        }
        if optimizer:
            checkpoint['optimizer'] = optimizer.state_dict()
        if scheduler:
            checkpoint['scheduler'] = scheduler.state_dict()
        if scheduler:
            checkpoint['grad_scaler'] = grad_scaler.state_dict()

        torch.save(checkpoint, filename)
        logging.info("=> Saving checkpoint complete.")

    @staticmethod
    def load_checkpoint(
            model: nn.Module,
            filename: str,
            optimizer: optim.Optimizer = None,
            scheduler: optim.lr_scheduler = None,
            grad_scaler: GradScaler = None,
    ) -> int:
        logging.info("=> Loading checkpoint")
        checkpoint = torch.load(filename, map_location="cuda")
        model.load_state_dict(checkpoint['state_dict'], strict=False)
        if 'optimizer' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer'])
        if 'scheduler' in checkpoint:
            scheduler.load_state_dict(checkpoint['scheduler'])
        if 'grad_scaler' in checkpoint:
            grad_scaler.load_state_dict(checkpoint['grad_scaler'])
        return checkpoint['epoch']


class CustomImageTextDataset(Dataset):
    def __init__(
            self,
            image_dir: str,
            pooled_embedding_dir: str,
            token_embedding_dir: str,
            token_mask_dir: str,
            image_size: int,
    ):
        self.image_dir = image_dir
        self.pooled_embedding_dir = pooled_embedding_dir
        self.token_embedding_dir = token_embedding_dir
        self.token_mask_dir = token_mask_dir

        self.image_name_list = os.listdir(image_dir)
        self.pooled_embedding_name_list = os.listdir(pooled_embedding_dir)
        self.token_embedding_name_list = os.listdir(token_embedding_dir)
        self.token_mask_name_list = os.listdir(token_mask_dir)

        print(len(self.image_name_list), len(self.pooled_embedding_name_list))

        assert len(self.image_name_list) == len(self.pooled_embedding_name_list), \
            'Images and pooled text embedding count should be same.'
        assert len(self.image_name_list) == len(self.token_embedding_name_list), \
            'Images and text token embedding count should be same.'
        assert len(self.image_name_list) == len(self.token_mask_name_list), \
            'Number of images and token masks count should be same.'

        self.image_transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=(0.5, 0.5, 0.5),
                std=(0.5, 0.5, 0.5),
            )
        ])

    def __len__(self) -> int:
        return len(self.image_name_list)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, ...]:
        file_name, file_ext = os.path.splitext(self.image_name_list[idx])

        image = Image.open(os.path.join(self.image_dir, self.image_name_list[idx]))
        pooled_text_embedding = np.load(os.path.join(self.pooled_embedding_dir, f'pooled_embed_{file_name}.npy'))
        token_text_embedding = np.load(os.path.join(self.token_embedding_dir, f'token_{file_name}.npy'))
        token_mask = np.load(os.path.join(self.token_mask_dir, f'mask_{file_name}.npy'))

        image = self.image_transform(image)
        pooled_embedding = torch.from_numpy(pooled_text_embedding)
        token_embedding = torch.from_numpy(token_text_embedding)
        token_mask = torch.from_numpy(token_mask)

        return (
            image,
            pooled_embedding,
            token_embedding,
            token_mask
        )


class TimestepPositionalEncoding(nn.Module):
    def __init__(
            self,
            embedding_dim: int,
            dropout: float = 0.1,
            max_len: int = 1000,
    ):
        """Section 3.5 of attention is all you need paper.

        Extended slicing method is used to fill even and odd position of sin, cos with increment of 2.
        Ex, `[sin, cos, sin, cos, sin, cos]` for `embedding_dim = 6`.

        `max_len` is equivalent to number of noise steps or patches. `embedding_dim` must same as image
        embedding dimension of the model.

        Args:
            embedding_dim: `d_model` in given positional encoding formula.
            dropout: Dropout amount.
            max_len: Number of embeddings to generate. Here, equivalent to total noise steps.
        """
        super(TimestepPositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pos_encoding = torch.zeros(max_len, 1, embedding_dim)
        position = torch.arange(start=0, end=max_len).unsqueeze(1)
        div_term = torch.exp(-math.log(10000.0) * torch.arange(0, embedding_dim, 2).float() / embedding_dim)

        pos_encoding[:, 0, 0::2] = torch.sin(position * div_term)
        pos_encoding[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer(name='pos_encoding', tensor=pos_encoding, persistent=False)

    def forward(self, t: torch.LongTensor) -> torch.Tensor:
        """Get precalculated positional embedding at timestep t. Outputs same as video implementation
        code but embeddings are in [sin, cos, sin, cos] format instead of [sin, sin, cos, cos] in that code.
        Also batch dimension is added to final output.
        """
        positional_encoding = self.pos_encoding[t]
        return self.dropout(positional_encoding)


class TransformerEncoder(nn.Module):
    def __init__(
            self,
            dim: int,
            num_blocks: int = 1,
            mlp_hidden_dim: int = 1024,
            num_head: int = 8,
            per_dim_head: int = 64,
            dropout: float = 0.1,
    ):
        """Runs embedded patches through transformer encoder. Figure 1 of ViT paper.
        """
        super(TransformerEncoder, self).__init__()

        self.blocks = nn.ModuleList()
        for _ in range(num_blocks):
            self.blocks.append(
                nn.ModuleList([
                    nn.LayerNorm([dim]),
                    FlashAttention(dim=dim, heads=num_head, dim_head=per_dim_head),
                    nn.Sequential(
                        nn.LayerNorm([dim]),
                        nn.Linear(in_features=dim, out_features=mlp_hidden_dim),
                        nn.GELU(),
                        nn.Dropout(p=dropout),
                        nn.Linear(in_features=mlp_hidden_dim, out_features=dim),
                        nn.Dropout(p=dropout),
                    ),
                ])
            )

    def forward(self, x: torch.Tensor, context: torch.Tensor = None, mask: torch.Tensor = None) -> torch.Tensor:
        for layer_norm, attn, mlp in self.blocks:
            x = attn(x=layer_norm(x), context=context, mask=mask) + x
            x = mlp(x) + x
        return x


class ViT(nn.Module):
    def __init__(
            self,
            dim: int,
            image_size: int,
            patch_size: int,
            emb_dropout: int = 0.1,
            img_channels: int = 3,
            num_blocks: int = 2,
            **kwargs,
    ):
        """ViT implementation without class token. Assumes square input image tensor in shape of (b, c, h, w) and
         square patch size. Uses learned 1D positional encoding.
        """
        super(ViT, self).__init__()

        patch_tmp = image_size // patch_size
        self.num_patches = patch_tmp ** 2
        patch_dim = img_channels * patch_size * patch_size

        self.to_patch_embedding = nn.Sequential(
            Rearrange('b c (h ph) (w pw) -> b (h w) (ph pw c)', ph=patch_size, pw=patch_size),
            nn.LayerNorm([patch_dim]),
            nn.Linear(in_features=patch_dim, out_features=dim),
            nn.LayerNorm([dim]),
        )

        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, dim))
        self.dropout = nn.Dropout(p=emb_dropout)

        self.transformer_encoder = TransformerEncoder(
            dim=dim,
            num_blocks=num_blocks,
            **kwargs,
        )

        self.to_img_pixels = nn.Sequential(
            nn.LayerNorm([dim]),
            nn.Linear(in_features=dim, out_features=patch_dim),
            Rearrange(
                'b (h w) (ph pw c) -> b c (h ph) (w pw)',
                ph=patch_size,
                pw=patch_size,
                c=img_channels,
                h=patch_tmp,
                w=patch_tmp,
            ),
        )

    def forward(self, x: torch.Tensor, context: torch.Tensor = None, mask: torch.Tensor = None) -> torch.Tensor:
        x = self.to_patch_embedding(x)
        x += self.pos_embedding
        x = self.dropout(x)
        x = self.transformer_encoder(x=x, context=context, mask=mask)
        return self.to_img_pixels(x)


v = torch.randn((4, 8, 256, 256)).cuda()
m = ViT(num_blocks=2, dim=768, image_size=256, patch_size=16, img_channels=8).cuda()
out = m(v)
print(out.shape)
